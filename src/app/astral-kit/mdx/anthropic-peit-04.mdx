---
title: 'Separating Data from Instructions'
subtitle: 'Chapter 4'
type: 'tutorial'
stage: 'draft'
topics: ['Anthropic', 'Prompt Engineering', 'Intermediate']
createdAt: '2024-04-19'
updatedAt: '2024-05-07'
---

## Lesson

Oftentimes, we don't want to write full prompts, but instead want **prompt templates that can be modified later with additional input data before submitting to Claude**. This might come in handy if you want Claude to do the same thing every time, but the data that Claude uses for its task might be different each time.

Luckily, we can do this pretty easily by **separating the fixed skeleton of the prompt from variable user input**, then substituting the user input into the prompt before sending the full prompt to Claude.

Below, we'll walk step by step through how to write a substitutable prompt template, as well as how to substitute in user input.

## Examples

In this first example, we're <Mark>asking Claude to act as an animal noise generator</Mark>. The full prompt submitted to Claude is just the prompt template (first box) substituted with the input (in this case, "Cow", in the second box).

**Note: You don't have to call your substitution placeholder anything in particular** in practice. For this example, definitely use `{{ANIMAL}}`, as that's how the exercise is formatted. But in general, just as easily, we could have called it `{{CREATURE}}` or `{{A}}` (but it's **generally good to have your stand-ins be specific and relevant** so that your prompt is easy to understand even without the substitution, just for user parseability). Just make sure that whatever you name your substitution placeholder is what you use for the substitution formula.

<Playground
  user={`I will tell you the name of an animal. Please respond with the noise that animal makes. {{ANIMAL}}`}
  assistant={`Moo.`}
  labels={{ user: 'Prompt Template', assistant: "Claude's Response" }}
  input={{ ANIMAL: 'Cow' }}
/>

Why would we want to separate and substitute inputs like this? Well, **prompt templates simplify repetitive tasks**. Let's say you build a prompt structure that invites third party users to submit content to the prompt (in this case the animal whose sound they want to generate). These third party users don't have to write or even see the full prompt. All they have to do is fill in variables.

We use the `{{double-curly-brackets}}` formatting in our own code as well.

**Note:** <Mark>Prompt templates can have as many variables as desired</Mark>.

When introducing substitution variables like this, it is very important to <Mark>make sure Claude knows where variables start and end</Mark> (vs. instructions or task descriptions). Let's look at an example where there is no separation between the instructions and the substitution variable.

<Playground
  user={`Yo Claude. {{EMAIL}} <----- Make this email more polite but don't change anything else about it.`}
  assistant={`Dear Claude,

I hope this email finds you well. As the CEO, I would like to request your presence at our office tomorrow at 6 am. I understand this may be an early start, but I believe it is necessary for the success of our company. Please let me know if you have any concerns or conflicts with this schedule, and we can work together to find a suitable arrangement.

Thank you for your dedication and hard work. I look forward to seeing you tomorrow.

Best regards,\n[Your CEO's Name]`}
  labels={{ user: 'Prompt Template', assistant: "Claude's Response" }}
  input={{ EMAIL: `Show up at 6am tomorrow because I'm the CEO and I say so.` }}
/>

Here, <Mark>Claude thinks "Yo Claude" is part of the email it's supposed to rewrite</Mark>! You can tell because it begins its rewrite with "Dear Claude". To the human eye, it's clear, particularly in the prompt template where the email begins and ends, but it becomes much less clear in the prompt after substitution.

How do we solve this? **Wrap the input in XML tags**! We did this below, and as you can see, there's no more "Dear Claude" in the output.

[XML tags](https://docs.anthropic.com/claude/docs/use-xml-tags) are angle-bracket tags like `<tag></tag>`. They come in pairs and consist of an opening tag, such as `<tag>`, and a closing tag marked by a "/", such as `</tag>`. XML tags are used to wrap around content, like this: `<tag>`content`</tag>`.

**Note:** While Claude can recognize and work with a wide range of separators and delimeters, we recommend that you **use specifically XML tags as separators for Claude**, as Claude was trained specifically to recognize XML tags as a prompt organizing mechanism. Outside of function calling, **there are no special sauce XML tags that Claude has been trained on that you should use to maximally boost your performance**. We have purposefully made Claude very malleable and customizable this way.

<Playground
  user={`Yo Claude. <email>{{EMAIL}}</email> <----- Make this email more polite but don't change anything else about it.`}
  assistant={`Dear [Your Name],

I hope this email finds you well. As the CEO, I would like to request your presence at 6am. I understand this may be an early start, but I believe it is necessary for the success of our organization. Please let me know if this time works for you, and I appreciate your cooperation.

Best regards,\n[CEO's Name]`}
  labels={{ user: 'Prompt Template', assistant: "Claude's Response" }}
  input={{ EMAIL: `Show up at 6am because I'm the CEO and I say so.` }}
/>

Let's see another example of how XML tags can help us.

In the following prompt, <Mark>Claude incorrectly interprets what part of the prompt is the instruction vs. the input</Mark>. It incorrectly considers "Each is about an animal, like rabbits" to be part of the list due to the formatting, when the user (the one filling out the `{{SENTENCES}}` variable) presumably did not want that.

<Playground
  user={`Below is a list of sentences. Tell me the second item on the list.\n
- Each is about an animal, like rabbits.
{{SENTENCES}}`}
  assistant={`The second item on the list is "I like how cows sound".`}
  labels={{ user: 'Prompt Template', assistant: "Claude's Response" }}
  input={{
    SENTENCES: `- I like how cows sound
- This sentence is about spiders
- This sentence may appear to be about dogs but it's actually about pigs`,
  }}
/>

To fix this, we just need to <Mark>surround the user input sentences in XML tags</Mark>. This shows Claude where the input data begins and ends despite the misleading hyphen before "Each is about an animal, like rabbits."

<Playground
  user={`Below is a list of sentences. Tell me the second item on the list.\n
- Each is about an animal, like rabbits.
  <sentences>{{ SENTENCES }}</sentences>`}
  assistant={`The second item on the list is "This sentence is about spiders".`}
  labels={{ user: 'Prompt Template', assistant: "Claude's Response" }}
  input={{
    SENTENCES: `- I like how cows sound
- This sentence is about spiders
- This sentence may appear to be about dogs but it's actually about pigs`,
  }}
/>

**Note:** In the incorrect version of the "Each is about an animal" prompt, we had to include the hyphen to get Claude to respond incorrectly in the way we wanted to for this example. This is an important lesson about prompting: **small details matter**! It's always worth it to **scrub your prompts for typos and grammatical errors**. Claude is sensitive to patterns (in its early years, before finetuning, it was a raw text-prediction tool), and it's more likely to make mistakes when you make mistakes, smarter when you sound smart, sillier when you sound silly, and so on.

If you're ready to try the chapter exercises, scroll down!

## Exercises

### Haiku Topic

Write a prompt in the user box that will <Mark>take in a variable called `{{TOPIC}}` and output a haiku</Mark> about the topic. This exercise is just meant to test your understanding of the variable templating structure.

<Playground
  user={`Write a haiku about {{TOPIC}}.`}
  assistant={`Here is a haiku about pigs:\n
Snouts in the mud, snort
Curly tails, pink and plump, oink
Pigs roam the farmyard`}
  labels={{ user: 'Prompt Template', assistant: "Claude's Response" }}
  input={{ TOPIC: `Pigs` }}
  exercise={{ eval: '04-haiku-topic', answers: ['user'] }}
/>

<Details>
  <summary>If you need a hint, click me.</summary>

Don't forget to include the exact phrase `{{TOPIC}}` wherever you want the topic to be substituted in.

</Details>

### Dog Question with Typos

<Mark>Fix the prompt in the User box by adding XML tags</Mark> so
that Claude produces the right answer.

Try not to change anything else about the prompt. The messy and mistake-ridden writing is intentional, so you can see how Claude reacts to such mistakes. Claude's response will have a green check mark on it if your prompt produces the right answer.

**Note:** Although Claude can somewhat parse through broken prompts with poor grammar and misspelled words, **having a clearly written, spell-checked and grammatically correct prompt far decreases the risk of Claude making mistakes and far increases the quality of Claude's output**.

<Playground
  user={`Hia its me i have a q about dogs jkaerjv <question>{{QUESTION}}</question> jklmvca tx it help me muhch much atx fst fst answer short short tx`}
  assistant={`Yes, dogs can be brown.`}
  labels={{ user: 'Prompt Template', assistant: "Claude's Response" }}
  input={{ QUESTION: `ar cn brown?` }}
  exercise={{
    eval: '04-dog-question-with-typos',
    answers: ['user'],
    initialUser:
      'Hia its me i have a q about dogs jkaerjv {{QUESTION}} jklmvca tx it help me muhch much atx fst fst answer short short tx',
  }}
/>

<Details>
  <summary>If you need a hint, click me.</summary>

Hint: If you surround `{{QUESTION}}` in XML tags, how does that change Claude's response?

This exercise is looking for a response that affirms dogs can be brown.

</Details>

### Dog Question Part 2

Fix the prompt in the User box <Mark>**WITHOUT** adding XML tags</Mark>. Instead, <Mark>remove only one or two words from the prompt</Mark>.

Just as with the above exercises, try not to change anything else about the prompt. This will show you what kind of language Claude can parse and understand. Claude's response will have a green check mark if your prompt produces the right answer.

<Playground
  user={`Hia its me i have a q about dogs {{QUESTION}} tx it help me muhch much atx fst fst answer short short tx`}
  assistant={`Yes, dogs can be brown.`}
  labels={{ user: 'Prompt Template', assistant: "Claude's Response" }}
  input={{ QUESTION: `ar cn brown?` }}
  exercise={{
    eval: '04-dog-question-with-typos',
    answers: ['user'],
    initialUser:
      'Hia its me i have a q about dogs jkaerjv {{QUESTION}} jklmvca tx it help me muhch much atx fst fst answer short short tx',
  }}
/>

<Details>
  <summary>If you need a hint, click me.</summary>

Try removing one word or section of characters at a time, starting with the parts that make the least sense. Doing this one word at a time will also help you see just how much Claude can or can't parse and understand.

This exercise is looking for a response that affirms dogs can be brown.

</Details>

<Pagination
  prev={{
    href: './anthropic-peit-03',
    title: 'Assigning Roles (Role Prompting)',
    subtitle: 'Chapter 3',
  }}
  next={{
    href: './anthropic-peit-05',
    title: 'Formatting Output & Speaking for Claude',
    subtitle: 'Chapter 5',
  }}
  top={{
    href: '/astral-kit/anthropic-peit',
    title: 'Prompt Engineering Interactive Tutorial',
  }}
/>
